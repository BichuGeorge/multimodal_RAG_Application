{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "load_dotenv()\n",
    "mistral_api_key = os.environ[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model\n",
    "model = \"pixtral-12b-2409\"\n",
    "\n",
    "# Initialize the Mistral client\n",
    "mistral_model = Mistral(api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal RAG APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\anaconda3\\envs\\multimodal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "input_path = os.getcwd()\n",
    "output_path = os.path.join(\n",
    "    os.getcwd(),\n",
    "    \"figures\"\n",
    ")\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(\n",
    "        input_path,\n",
    "        \"startupai-financial-report-v2.pdf\"\n",
    "    ),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x1b1b9d71c30>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the relevant info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the text, images and tables in 3 lists\n",
    "we need to send images in binary format using base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of table elements in the pdf file: 0\n",
      "Number of text elements in the pdf file: 1\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(\n",
    "            image_file.read(),\n",
    "        ).decode('utf-8')\n",
    "    \n",
    "for element in raw_pdf_elements:\n",
    "    if \"CompositeElement\" in str(type(element)):\n",
    "        text_elements.append(element)\n",
    "    elif \"Table\" in str(type(element)):\n",
    "        table_elements.append(element)\n",
    "\n",
    "table_elements = [i.text for i in table_elements]\n",
    "text_elements = [i.text for i in text_elements]\n",
    "\n",
    "print(f\"Number of table elements in the pdf file: {len(table_elements)}\")\n",
    "print(f\"Number of text elements in the pdf file: {len(text_elements)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are currently stored in figured folder\n",
    "we will encode it using base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image elements in the pdf file: 6\n"
     ]
    }
   ],
   "source": [
    "for image_file in os.listdir(output_path):\n",
    "    if image_file.endswith((\".png\",\".jpg\",\".jped\")):\n",
    "        image_path = os.path.join(output_path, image_file)\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_elements.append(encoded_image)\n",
    "print(f\"Number of image elements in the pdf file: {len(image_elements)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 3 functions to summarize images, tables and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "groq_key = os.getenv(\"GROQ\")\n",
    "groq_model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "def summarize_text(text_element):\n",
    "    prompt=f\"Summarize the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = groq_model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=prompt\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "def summarize_table(table_element):\n",
    "    prompt=f\"Summarize the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = groq_model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=prompt\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "def summarize_image(encoded_image):\n",
    "    prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Describe the contents of this image\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                    }\n",
    "                }\n",
    "        ]\n",
    "    }        \n",
    "]\n",
    "    try: \n",
    "        # Get the chat response\n",
    "        chat_response = mistral_model.chat.complete(\n",
    "            model=model,\n",
    "            messages=prompt\n",
    "        )\n",
    "    except Exception as e:\n",
    "        import time\n",
    "        time.sleep(6)\n",
    "\n",
    "    # Print the content of the response\n",
    "    return chat_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of texts processed\n",
      "1th element of image processed\n",
      "2th element of image processed\n",
      "3th element of image processed\n",
      "4th element of image processed\n",
      "5th element of image processed\n",
      "6th element of image processed\n"
     ]
    }
   ],
   "source": [
    "text_summaries = []\n",
    "for i, te in enumerate(text_elements[0:2]):\n",
    "    summary = summarize_text(te)\n",
    "    text_summaries.append(summary)\n",
    "    print(f\"{i+1}th element of texts processed\")\n",
    "\n",
    "tabe_summaries = []\n",
    "for i, te in enumerate(table_elements[0:2]):\n",
    "    summary = summarize_table(te)\n",
    "    tabe_summaries.append(summary)\n",
    "    print(f\"{i+1}th element of table processed\")\n",
    "\n",
    "image_summaries = []\n",
    "for i, te in enumerate(image_elements[0:10]):\n",
    "    summary = summarize_image(te)\n",
    "    image_summaries.append(summary)\n",
    "    print(f\"{i+1}th element of image processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will proceed with the RAG technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bichu George\\AppData\\Local\\Temp\\ipykernel_13824\\1882708438.py:9: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding_function=HuggingFaceEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "vectorstorev2 = Chroma(collection_name=\"summaries\",\n",
    "                       embedding_function=HuggingFaceEmbeddings())\n",
    "storev2 = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retrieverv2 = MultiVectorRetriever(\n",
    "    vectorstore=vectorstorev2,\n",
    "    docstore=storev2, id_key=id_key,\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(\n",
    "            page_content=s, metadata={\n",
    "                id_key: doc_ids[i]\n",
    "            }\n",
    "        )\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retrieverv2.vectorstore.add_documents(summary_docs)\n",
    "    retrieverv2.docstore.mset(\n",
    "        list(\n",
    "            zip(\n",
    "                doc_ids,\n",
    "                original_contents\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents_to_retriever(\n",
    "    text_summaries, text_elements\n",
    ")\n",
    "\n",
    "# add_documents_to_retriever(\n",
    "#     tabe_summaries, table_elements\n",
    "# )\n",
    "\n",
    "add_documents_to_retriever(\n",
    "    image_summaries, image_elements\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context, \n",
    "which can include text, images and tables: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "groq_model = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_key)\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve_context(query):\n",
    "    docs = retrieverv2.get_relevant_documents(query)  # Use correct method\n",
    "    return docs\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough() | retrieve_context,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | groq_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The name of the company is StartupAI.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"What is the name of the company?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
